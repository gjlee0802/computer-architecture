# 면접 기출 예시

## 개념 질문
### Memory Hierarchy
#### 1. ❓캐시가 필요한 이유는? Cache hit ratio 에 대해 설명하시오.
* 캐시가 필요한 이유: 
~~~
캐시는 데이터 접근 속도를 높이고, 시스템의 성능을 최적화하기 위해 사용
즉, 빠른 메모리에 자주 사용하는 데이터를 저장해서 성능을 높이자는 원칙
~~~
* Cache hit ratio란?:
~~~
Cache Hit Ratio(캐시 적중률)는 캐시에서 원하는 데이터를 찾을 확률을 나타내는 지표로, 
전체 요청 중 캐시에서 성공적으로 제공된 요청의 비율을 의미함

Cache Hit Ratio = Cache Hits / (Cache Hits + Cache Misses)
~~~

-----

#### 2. ❓메모리 접근하는데 x 사이클이 걸리고 캐시에 접근하는데 y 사이클이 걸리며 캐시 hit rate 가 h %일 때 effective access time은?
~~~
EAT (Effective Access Time)은 cache를 활용한 메모리 접근의 평균 소요 시간을 계산하는 값
~~~
~~~
Cache Hit rate와 Miss rate를 고려하여 다음과 같이 계산함:

EAT = (Hit Rate x Cache Access Time) + (Miss Rate x (Cache Access Time + Memory Access Time))-
    = Cache Access Time + Miss Rate x Memory Access Time
~~~
🎯 즉, `캐시 접근 시간 y`에 `Cache Miss 발생 시 추가로 걸리는 메모리 접근 시간`이 더해지는 형태!  

-----

#### 3. ❓Page Fault는 언제 발생하는가? Page Fault 비율과 Cache Miss 비율 중 큰 것은? 그 이유는?
**Page Fault는 언제 발생하는가?:**
~~~
🎯 Page Fault는 Page Table에서 해당 페이지의 Valid Bit이 0으로 설정되어 있어, 해당 페이지가 물리 메모리에 존재하지 않는 경우 발생함  
✅ 이때 운영체제(OS)는 디스크에서 해당 페이지를 가져와 메모리에 로드하는 과정(Page Swap-In)을 수행함  
~~~
**Page Fault 비율과 Cache Miss 비율 중 큰 것은? 그 이유는?:**
~~~
🎯 일반적으로 Cache Miss 비율이 Page Fault 비율보다 훨씬 큼  
✅ 이유: 캐시는 작은 용량에서 빠르게 동작하며 자주 교체되지만, 
    페이지는 운영체제가 효율적으로 관리하여 디스크 접근을 최소화하기 때문  
~~~

-----

#### 4. ❓Cache Miss의 3 종류 (3C)를 나열하고 각각에 대해 Miss를 낮추는 방법을 설명하시오.
~~~
1️⃣ Conflict Misses (충돌 미스): 
    ✅ 원인: Set 내부에서의 Cache Entry끼리 경쟁으로 발생
    ✅ 감소 방법: Fully Associative (Associative, Set의 개수 증가)

2️⃣ Compulsory(aka. Cold Start) Misses (필수 미스): 
    ✅ 원인: Block에 처음 접근할 때는 비어있어, Miss가 필수적으로 발생
    ✅ 감소 방법: Block의 크기 증가

3️⃣ Capacity Misses (용량 미스): 
    ✅ 원인: 캐시 크기(용량)가 부족해서 기존의 Block이 강제로 제거됨
    ✅ 감소 방법: 캐시 크기 증가
~~~

-----

#### 5. Cache는 보편적으로 프로그램의 메모리 접근 패턴에 존재하는 두 가지의 지역성 (Locality)를 이용한다. 두 가지 지역성의 명칭과 각각 어떤 특성에 대한 것인지 설명하시오.
~~~
1️⃣ Spatial Locality (공간적 지역성): 
    최근에 접근한 주소와 인접한 메모리에 접근할 확률이 높다는 특성

2️⃣ Temporal Locality (시간적 지역성): 
    최근에 접근한 데이터를 곧(가까운 미래에) 다시 접근할 확률이 높다는 특성
~~~

-----

#### 6. Direct mapped cache, fully associative cache, set-associative cache의 구조를 각기 설명하고, (1) cache miss ratio, (2) cache latency의 2가지 측면에서 각각의 cache 구조가 갖는 장단점을 논하시오.
🎯 **각 캐시의 구조**:  
* 1️⃣ Direct Mapped Cache:
    ~~~
    구조: 각 block은 오직 하나의 cache line에만 매핑될 수 있음
    Indexing: 주소의 일부 비트로 cache line 결정, 해당 위치의 tag와만 수행함
    ~~~
* 2️⃣ Fully Associative Cache:
    ~~~
    구조: block이 어느 위치에든 저장 가능
    Indexing: Indexing 없음, 모든 Tag를 비교
    ~~~
* 3️⃣ Set Associative Cache:
    ~~~
    구조: 한 block이 정해진 여러 개의 set 중 하나에 저장 가능
    Indexing: Indexing으로 set을 정하고, 해당 set 내부에서 Tag 비교
    ~~~

🎯 **Cache Miss Ratio 관점**: Fully Associative < Set-Associative < Direct Mapped  
| 구조                 | 장점                                     | 단점                                                  |
|----------------------|------------------------------------------|--------------------------------------------------------|
| **Direct Mapped**    | 단순하고 빠름                            | **Conflict miss 발생률 높음** (같은 index에 충돌 발생) |
| **Fully Associative**| **Conflict miss 없음 → 가장 낮은 miss ratio** | 구현 복잡, 비용 큼                                     |
| **Set-Associative**  | Conflict miss 줄어듦 (associativity에 따라) | associativity가 낮으면 conflict miss 가능             |

🎯 **Cache Latency 관점**: Direct Mapped < Set-Associative < Fully Associative
| 구조                 | 장점                                 | 단점                                              |
|----------------------|--------------------------------------|---------------------------------------------------|
| **Direct Mapped**    | **가장 빠름** (단일 위치만 비교)     | 유연성 낮음                                       |
| **Fully Associative**| 유연성 높음                          | **가장 느림** (모든 entry 비교 → 비교기 필요)     |
| **Set-Associative**  | 타협적 속도 (set 내에서만 비교)      | associativity 높을수록 latency 증가               |

-----

### Virtual Memory
#### 1. ❓캐시 메모리와 메인 메모리의 주소 지정 방식의 차이점이 무엇인가?
~~~
🎯 캐시는 CPU가 메모리에 접근할 때 Physical Address를 기반으로 접근하도록 함
    반면, 프로세스가 사용하는 메인 메모리는 Virtual Address를 기반으로 접근 
    (OS가 Virtual Address를 Physical Address로 매핑하여 메인메모리를 관리)
~~~

-----

#### 2. ❓TLB란 무엇인가?
~~~
TLB: Virtual Address를 Physical Address로 변환할 때 사용하는 Cache 장치로, 매번 메인메모리의 Page Table을 탐색하지 않아도 되도록 하기 위해 최근에 Translation에 사용한 주소 Mapping 정보를 보관함
~~~

## 💪 Cache Performance 관련 심층 문제
### 1. ❓Level 1 Cache(Primary Cache)가 아래와 같은 Miss Rate로 동작한다고 하자. Miss rate = 0%인 Perfect cache가 있다고 가정할 때, 아래의 Cache 대비 성능 향상 Factor을 계산하시오. (SSU 17년도 기출)
**주어진 Cache의 변수:**
~~~
Instruction cache miss rate = 2%
Data cache miss rate = 4%
CPI = 1 without any memory stalls
Miss penalty = 100 cycles
Frequency of all loads and stores = 25%
~~~

**Actual CPI 계산:**
~~~
Base CPI = 1
I-Cache Miss Cycle = Instruction Cache Miss Rate x Miss Penalty 
    = 0.02 x 100 = 2 Cycle
D-Cache Miss Cycle = Data Cache Miss Rate x 0.25 x Miss Penalty 
    = 0.04 x 0.25 x 100 = 1 Cycle
Actual CPI = Base CPI + I-Cache Miss Cycle + D-Cache Miss Cycle 
    = 1 + 2 + 1 = 4 Cycle
~~~
따라서 Perfect Cache는 위의 Cache 성능(Actual CPI)에 비해 4배의 성능 향상이 있음

-----

### 2. ❓Multilevel Cache 성능 비교이다. (SSU 17년도 기출)

**(a) 주어진 Level-1 Cache(Primary Cache)의 변수:**
~~~
Clock rate = 4 GHz
CPI = 1.0 with a primary cache of 100% hit rate
Main memory access time = 100 ns (including miss handling)
Miss rate/instruction at the primary cache = 2 %
~~~

**(b) 주어진 Level-2 Cache의 변수:**
~~~
Miss rate (to Main Memory) = 0.6 %
L2 cache access time = 5 ns
~~~

#### 2.1. ❓Primary cache(L1 cache)만을 사용할 경우 Total CPI(Effective CPI)를 구하시오. 
🎯 `Total CPI` = `Base CPI` + `Miss CPI` = `Base CPI` + (`Miss Rate/Instruction` x `Miss Penalty`)  
✅ 여기서는 Miss Penalty를 계산하는 것이 중요 Point!  
✅ `L1 Cache Miss Penalty` = `Memory Access Time` / `Clock Cycle Time`  

✅ **풀이:**  
~~~
Base CPI = 1
Clock rate = 4GHz가 주어지면, =>  1 Clock cycle 소요 시간 계산 떠올리기
1 Clock cycle time = 1 / Clock rate = 1 / 4GHz = 0.25 ns
L1 cache Miss penalty = Memory Access Time / Clock Cycle Time = 100 ns / 0.25 ns = 400 Clock cycles
~~~
🎯 따라서 Total CPI(Effective CPI)는 아래와 같이 계산됨
~~~
Total CPI = 1 + (0.02 x 400) = 1 + 8 = 9 Clock cycles
~~~

#### 2.2. ❓L2 cache까지 사용할 경우 Total CPI(Effective CPI)를 구하시오.
🎯 `Total CPI` = `Base CPI` + `L1 Miss with L2 Hit CPI` + `L1 Miss with L2 Miss CPI`  
✅ 여기서는 L2 Cache에서 Hit일 때와 Miss일 경우의 L1 Miss CPI를 나누어 생각하는 것이 중요 Point!  
✅ Primary Stalls: `L1 Miss with L2 Hit CPI` = `Miss rate at the primary cache` x `L1 Miss with L2 Hit Penalty`  
✅ Secondary Stalls: `L1 Miss with L2 Miss CPI` = `L2 Miss rate (to Main Memory)` x `L1 Miss with L2 Miss Penalty`  

✅ **풀이:**  
~~~
1 Clock cycle time = 1 / Clock rate = 1 / 4GHz = 0.25 ns

(1). L1 Miss with L2 Hit Penalty 계산하기 (L2 Access에 대한 Penalty 계산)
L1 Miss with L2 Hit Penalty = L2 cache access time / Clock cycle time
    = 5 / 0.25 = 20 Clock cycles

(2). L1 Miss with L2 Miss Penalty 계산하기 (Main Memory Access에 대한 Penalty 계산)
L1 Miss with L2 Miss Penalty = Memory access time / Clock cycle time
    = 100 / 0.25 = 400 Clock cycles
~~~
🎯 따라서 Total CPI(Effective CPI)는 아래와 같이 계산됨  
~~~
Total CPI = Base CPI + Primary Stalls + Secondary Stalls
    = 1 + (2% x 20) + (0.6% x 400) = 3.8 Clock cycles
~~~

#### 2.3. ❓L2 cache까지 사용할 경우, L1 cache만을 사용할 경우 대비 성능 향상 Factor을 계산하시오.
🎯 Primary cache만을 사용할 경우에는 Total CPI가 9, L-2 cache까지 사용할 경우 Total CPI가 3.8이었으므로, 2.36배의 성능 향상을 보임  
~~~
9 / 3.8 = 2.36
~~~

-----

### 3. ❓TLB, Cache, Virtual Memory를 통합하여 사용한다고 하자. 그림은 CPU가 Virtual Address로 Memory Access를 요청했을 경우 처리되는 경우들을 나누어 보여준다. (SSU 19년도 기출)
![cache_tlb_virtualmem](../image_files/cache_tlb_virtualmem.jpg)  

#### 3.1. ❓(E), (F), (I)의 Box 안 내용은 각각 무엇인가?
~~~
* (E): Lookup Page Table
* (F): Page Fault
* (I): Page Fault Exception => Page Fault Handler
~~~

#### 3.2. ❓실제 Physical Address로 Translation이 이루어지는 구간들은 어디인가? (예시: A와 B 사이 구간)
🎯 Cache에 접근하게 되는 구간에서 Virtual Address => Physical Address의 Translation이 이루어짐  
~~~
* B와 C 사이 구간
* F와 G 사이 구간
~~~
#### 3.3. ❓TLB Access Time = 10ns, Cache Access Time = 20ns, Main Memory Access Time = 100ns라고 하자.

**3.3.1. ❓그림 하단의 (0) ~ (3) 각각에 대해 메모리 접근 시간을 계산하시오.**
~~~
* (0): 10 + 20 = 30 ns
* (1): 10 + 20 + 100 = 130 ns
* (2): 10 + 100 + 20 = 130 ns
* (3): 10 + 100 + 20 + 100 = 230 ns
~~~

**3.3.2. ❓TLB Hit Rate = 99%, Cache Hit Rate = 80%라고 하자. 그림 하단의 (4)는 발생하지 않는다고 가정한다. CPU의 평균 메모리 접근시간을 위 3.3.1. 문제의 결과들을 이용하여 계산하시오. 메모리 참조 전체를 1로 간주할 때, (0) ~ (3) 각각이 차지하는 비율이 얼마인지를 먼저 계산하시오.**

**(0) ~ (3) 각각이 차지하는 비율 계산:** 
~~~
* (0): TLB Hit Rate x Cache Hit Rate = 0.99 x 0.8 = 0.79
* (1): TLB Hit Rate x Cache Miss Rate x 1 = 0.99 x 0.2 = 0.19
* (2): TLB Miss Rate x 1 x Cache Hit Rate = 0.01 x 0.8 = 0.008
* (3): TLB Miss Rate x 1 x Cache Miss Rate x 1 = 0.01 x 0.2 = 0.002
~~~

평균 메모리 접근 시간 계산:
~~~
(0)이 차지하는 비율 x (0)의 메모리 접근 시간 + (1)이 차지하는 비율 x (1)의 메모리 접근 시간 + (2)이 차지하는 비율 x (2)의 메모리 접근 시간 + (3)이 차지하는 비율 x (3)의 메모리 접근 시간

= (0.79 x 30) + (0.19 x 130) + (0.008 x 130) + (0.002 x 230) = 51 ns
~~~

-----

### 4. ❓아래의 설정 중, 어떤 것이 더 낮은 AMAT(Average Memory Access Time)을 갖고 있는가? 이유를 함께 설명하시오. (POSTECH 24-25 Second)
~~~
(1) A 32kB L1 cache, hit time of 1 cycle and miss time of 5 cycles, accessing a 256kB L2 cache. 
    The L1 hit rate is 70% and the L2 hit rate is 100%

(2) A 64kB L1 cache, hit time of 2 cycles and miss time of 4 cycles, accessing a 128kB L2 cache.
    The L1 hit rate is 90% and the L2 hit rate is 100%
~~~

✅ **풀이:**

**(1)의 AMAT**부터 계산해보자.
~~~
L2 hit rate가 100%이므로, L2 miss는 고려하지 않음
L1 hit rate가 70%이므로, L1 miss rate는 30%임

AMAT = L1 hit time + L1 miss rate x L1 miss pentalty
     = 1 + 0.3 x 5
     = 2.5 cycles
~~~

**(2)의 AMAT**를 계산해보자.
~~~
L2 hit rate가 100%이므로, L2 miss rate는 고려하지 않음
L1 hit rate가 90%이므로, L1 miss rate는 10%임

AMAT = L1 hit time + L1 miss rate x L1 miss penalty
     = 2 + 0.1 x 4
     = 2.4 Cycles
~~~
  
결론적으로,  
~~~
🎯 (2)의 (1)보다 더 낮은 AMAT를 갖고 있다.  
추가적으로, 캐시 용량은 AMAT 계산에는 직접적으로 영향을 주지 않지만,  
용량이 커지면, 대체로 Hit rate가 증가하고 Miss rate가 감소하는 경향이 있다.  
~~~

-----

### 5. ❓프로그램이 실행 사이클의 50%를 연산(Computation)에, 나머지 50%를 메모리 계층(Memory Hierarchy)을 통한 데이터 I/O에 사용한다고 가정하자. 시스템은 2단계 캐시 계층 구조(Two-level cache hierarchy)를 가지고 있으며, L1 캐시는 4사이클의 지연 시간(latency)과 10%의 미스율(miss rate)을 가진다. 마찬가지로, L2 캐시는 20사이클의 지연 시간과 20%의 미스율을 가지며, DRAM 접근은 평균적으로 100사이클이 소요된다.  (POSTECH 22-23 Second)

#### 5.1. ❓AMAT(Average Memory Access Time)은 몇 사이클인가?
✅ **AMAT 계산 공식(L2 miss를 고려한 일반식)**:  
~~~
AMAT = L1 hit time + L1 miss rate x (L2 hit time + L2 miss rate x Main Memory Access Time)
~~~

✅ **위에서 주어진 변수들**을 정리해보면,  
* L1 Cache
    * Hit time: 4 cycles
    * Miss rate: 10 %
* L2 Cache
    * Hit time: 20 cycles
    * Miss rate: 20 %
* Main memory
    * Access time: 100 cycles

🎯 AMAT를 계산해보면,  
~~~
AMAT = 4 + 0.1 x (20 + 0.2 x 100) = 8 cycles
~~~

✅ "50% 연산 + 50% 메모리 I/O"는 AMAT 계산 자체에는 사용되지 않음, 그러나 AMAT 개선이 전체 성능 향상에 끼치는 영향을 분석하는 데 꼭 필요함

#### 5.2. ❓동일한 비용 예산 내에서 전체 성능을 향상시키기 위해 다음 세 가지 후보 중 하나를 선택할 수 있다. 가장 빠른 시스템을 만들기 위해 어떤 옵션을 선택해야 하는가?
~~~
Option 1. 연산에 소요되는 실행 사이클을 20% 줄일 수 있도록 컴퓨팅 코어를 업그레이드한다.
Option 2. L1 캐시에 프리페처(prefetcher)를 도입하여 L1 및 L2 캐시의 미스율을 절반으로 줄인다.
Option 3. 평균 접근 지연 시간이 60사이클로 더 빠른 DRAM을 도입한다.
~~~

✅ **기준선: 기존 시스템의 전체 실행 시간**:
* 전체 실행 시간 = 연산 시간 + 메모리 I/O 시간
* 연산: 50%
* 메모리 I/O: 50%
* AMAT: 8 cycles

따라서, **전체 실행 시간 기준을** `100 cycles`이라고 **가정한다면**,
* 연산: 100 x 0.5 = 50 cycles
* 메모리: 100 x 0.5 x AMAT = 50 x 8 = 400 cycles
* 연산 + 메모리: 50 + 400 = 450 cycles

1️⃣ Option 1에 대한 성능 향상 계산
* 연산: 50 x 0.8 = 40 cycles
* 메모리: 400 cycles
* 연산 + 메모리: 40 + 400 = **440 cycles**

2️⃣ Option 2에 대한 성능 향상 계산
* 캐시 Miss rate가 달라졌으므로 AMAT를 다시 계산하면,
    ~~~
    AMAT = 4 + 0.05 x (20 + 0.1 x 100) = 4 + 0.05 x 30 = 5.5 cycles
    ~~~
* 연산: 50 cycles
* 메모리: 100 x 0.5 x AMAT = 50 x 5.5 = 275 cycles
* 연산 + 메모리: 50 + 275 = **325 cycles**

3️⃣ Option 3에 대한 성능 향상 계산
* Memory Access Time이 달라졌으므로 AMAT를 다시 계산하면,
    ~~~
    AMAT = 4 + 0.1 x (20 + 0.2 x 60) = 4 + 0.1 x 32 = 7.2 cycles
    ~~~
* 연산: 50 cycles
* 메모리: 100 x 0.5 x AMAT = 50 x 7.2 = 360 cycles
* 연산 + 메모리: 50 + 360 = **410 cycles**

🎯 결론적으로, 
~~~
Option 2을 선택해야 한다.
~~~

## 💪 Cache Associative 관련 심층 문제
### 1. ❓그림은 4 way set associative 방식 cache memory 구조를 보여준다. 각 data 크기는 4 byte이다. (SSU 18년도 기출)
![4-way_set_associative_cache](../image_files/4-way_set_associative_cache.png)  
#### 1.1. ❓그림에서 맨위 Address가 주어졌을 때, cache hit인지 판단하는 과정, hit일 경우 최종 data를 읽어내는 과정을 설명하시오.
1️⃣ **Step 1. 주소 분해** (Address Breakdown)
* 주어진 32-bit 주소에서:
    * Tag (22-bit): 상위 22비트
    * Index (8-bit): 중간 8비트 (256개의 index)
    * Offset (2-bit): 하위 2비트 (4-byte 데이터)

2️⃣ **Step 2. Index로 Cache 조회**
* Index 필드를 사용하여 해당 Index의 4개(4-way) 캐시 블록을 모두 조회
    * Index 하나당 4개의 캐시 블록이 조회됨

3️⃣ **Step 3. Tag 비교 및 유효성 체크**
* 각 way에서 저장된 Tag와 입력 Tag를 비교하고, Valid Bit가 1인지 확인
* 일치하는 Tag가 있고, 동시에 Valid Bit가 1이면 Cache Hit

4️⃣ **Step 4. Multiplexer을 통한 데이터 선택**
* Hit이 발생한 way의 데이터를 4-to-1 멀티플렉서를 사용하여 선택함
* 선택된 4 byte 데이터를 출력함

5️⃣ **Step 5. Hit 신호 출력**

#### 1.2. ❓data 부분의 cache memory의 총 크기는? (byte 단위로 표기할 것)
~~~
4 x 256 x 4 = 4096 bytes
~~~
#### 1.3. ❓data 부분의 cache memory의 총 크기는 위의 1.2.와 동일하고, 16 way set associative 방식으로 변경되어 운영한다고 하자. 맨 위에 주어진 실제 메모리 주소는 tag, index, block 내부 offset(B) (또는 block 내부 offset(W) 및 Word 내부 byte offset)으로 나뉜다. 각각 몇 bit인가?
1️⃣ **Step 1. Index의 크기(bits) 계산을 위해 Set의 개수 구하기**:
~~~
16 way set associative 방식은 한 Set 당 블록의 개수가 16개라는 의미

Data 부분의 총 크기 = 4096 bytes
Block(= Data 1개)의 크기 = 4 bytes

블록(aka. Cache Entry)의 개수 = 4096 / 4 = 1024 개

Set의 개수 = 블록(= Cache Entry) 개수 / Way 수
    = 1024 / 16 = 64개
~~~
2️⃣ **Step 2. Tag, Index, Block 내 Offset의 크기 계산하기**:
~~~
Index 크기 계산: 
2^6 = 64 (= Set의 개수)이므로, Index는 모든 Set의 개수를 포함할 수 있는 6 bits로 이루어짐

Byte 내 Offset 크기 계산: 
Byte Offset은 Block의 크기가 4 bytes이므로 2 bits로 이루어짐

Tag 크기 계산: 
Tag는 전체 32 bits에서 Index, Bytes Offset 부분을 제외한 24 bits로 이루어짐
~~~

#### 1.4. ❓그림은 CAM 에 해당하는가? CAM의 정의도 함께 설명하시오.
~~~
그림은 CAM에 해당한다.

CAM이란?:
Content Addressable Memory의 약자, 메모리 접근 시 주소를 갖고 탐색하는 것이 아니라 메모리에 저장된 내용으로 탐색 (비트 단위로 데이터를 저장하고, 입력된 키와 비교하여 일치하는 주소를 반환)

CAM은 Associative Memory의 한 형태이며, 하드웨어적으로 병렬 검색을 최적화한 버전이다.
~~~

-----

### 2. ❓64-byte 캐시가 주어졌다고 하자. 8-byte words의 데이터를 저장하려고 한다.
#### 2.1. ❓얼만큼의 word를 저장할 수 있는가?
~~~
64 / 8 = 8 words
~~~

#### 2.2. ❓이러한 words를 구분하고 접근하기 위해 얼만큼의 bits가 필요한가? 
캐시에 저장된 각 word를 구분하고 접근하기 위해 사용되므로, **Index bits에 해당**
~~~
2^3 = 8 이므로, 
words의 주소를 구분하기 위해 3 bits가 필요함
~~~

#### 2.3. ❓각 word 내 byte에 접근하기 위해 얼만큼의 bits가 필요한가?
word 내에서 특정 byte를 선택하기 위한 것이므로 **Offset bits에 해당**
~~~
각 word는 8 bytes를 갖기 때문에,
word 내 각 byte에 접근하기 위해 3 bits가 더 필요함
~~~

-----

### 3. ❓Consider an 8-way set-associative cache with 2MiB (data) capacity in a single-core 32-bit architecture CPU. The cache consists of 2048 sets. (POSTECH 22-23 First)

#### 3.1. ❓How many bits from memory address are used for tag, set index, and block offset, respectively?
✅ **block offset:**
~~~
8-way set-ssociative는 한 set 당 block의 개수가 8개라는 의미
data 부분의 총 크기는 2 Mebibytes 이므로 (2 x 1024 x 1024) bytes
block 1개의 크기('block size')를 모름
2 x 1024 x 1024 = 2048 x 8 x block size
block size = 2^7 = 128 bytes

block offset은 block 1개 크기인 128 bytes를 표현할 수 있어야 함
=> block offset은 7 비트
~~~

✅ **set index:**
~~~
2048 sets, 즉 2^11의 index 개수를 가질 수 있어야 함
=> set index는 11 비트
~~~

✅ **tag:**
~~~
tag를 위해서는 32-bit에서 set index와 block offset에 사용되는 비트를 제외한만큼 사용됨 (32 - 11 - 7)
=> tag는 14 비트
~~~

🎯 **결론적으로 정리하자면,**
~~~
tag: 14 bits | set index: 11 bits | block offset: 7 bits
~~~

🎯 **풀이 핵심:**
* block offset: 하나의 block 내 offset => 블록의 크기를 다 표현할 수 있는가?
* set index: 총 set의 개수를 다 표현할 수 있는가?
* tag: 전체 비트(32-bit)에서 block offset과 set index를 제외한 나머지 비트

#### 3.2. ❓Assuming a write-back cache, what is the cache’s storage overhead for metadata (e.g., tags, etc.) in KiB?
✅ **문제 의도:**
~~~
메타데이터의 오버헤드를 구하는 문제임 (=> 전체 메타데이터 크기 구하기)
Write-back cache이기 때문에 메타데이터인 valid bit, dirty bit, tag의 정보를 고려해야 함
전체 메타데이터 크기는 곧 전체 메타데이터 오버헤드를 의미
~~~
✅ **전체 block 수:**
~~~
# of blocks = 2048 sets x 8 blocks/set = 2^14 blocks
~~~
✅ **block 당 메타데이터:**
~~~
Tag bits: 앞 문제에서 계산한 대로 14 bits
Valid bit: 1 bit
Dirty bit: 1 bit

block 당 메타 데이터 개수는,
14 + 1 + 1 = 16 bits = 2 bytes per block
~~~
🎯 **전체 메타데이터 크기( = Metadata storage overhead):**
~~~
Metadata storage overhead = 2^14 blocks x 2 bytes = 2 ^ 15 bytes = 32 KiB
~~~

#### 3.3. ❓Consider making the following changes to the cache. Which of the 3 types of cache misses (i.e., cold miss, capacity miss, and conflict miss) would be affected by the change and how would they change (i.e., increase or decrease)? Assume the capacity of the cache stays the same. The changes are not cumulative.
1) Block size is increased.
2) Set-associativity is decreased.
3) A prefetcher with a separate prefetch buffer is implemented. Requests to the cache can now “hit” in the prefetch buffer. When a hit to the prefetch buffer occurs, the data is moved to the cache.

1️⃣ 1번. Block size가 증가하면,
~~~
Cold Miss 감소:
    한번에 더 많은 데이터를 불러와 Spatial Locality가 좋은 상황에서 Cold Miss 감소
~~~

2️⃣ 2번. Set-associativity가 감소하면,
~~~
Conflict Miss 증가:
    각 set에 들어갈 수 있는 block 수가 감소하고,
    같은 index로 매핑되는 주소들 간의 충돌(conflict) 증가함
~~~

3️⃣ 3번. Prefetch 버퍼를 사용하면,
~~~
Cold Miss 감소:
    프로그램에서 아직 접근하지 않은 데이터가 버퍼에 미리 로드되므로 
    첫 접근 시 miss가 발생하지 않을 수 있음

Capacity Miss 감소 가능:
    Prefetch buffer는 cache에 있는 데이터를 쫓아내지 않아도 데이터 유지 가능
~~~